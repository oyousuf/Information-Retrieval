{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "The words_before/after list would look like \n",
    "\n",
    "[['~'], '598'],\n",
    " ['-'], '143'],\n",
    " ... \n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "with open('index_before.txt', encoding=\"utf8\") as f:\n",
    "    # before lower casing\n",
    "    words_before = [line.split() for line in f.read().split('\\n') if line]\n",
    "    \n",
    "\n",
    "with open('index_after.txt', encoding=\"utf8\") as f:\n",
    "    # after lower casing \n",
    "    words_after = [line.split() for line in f.read().split('\\n') if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aboriginal-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52788\n",
      "50966\n"
     ]
    }
   ],
   "source": [
    "print(len(words_before)) # the length of words before lower casing - 52788\n",
    "print(len(words_after)) # the length of words after lower casing  - 50966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "employed-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to get the 10 most frequent words from the lowercased test collections\n",
    "def stop_words(dictionary, top_n_words): \n",
    "    stop_words = []\n",
    "    i = 0\n",
    "    for k in sorted(dictionary, key=lambda k: len(dictionary[k]), reverse=True):\n",
    "        if i < top_n_words:\n",
    "            stop_words.append(k)\n",
    "        i += 1\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def inverted_index(indexlist, stop_words_processing = False, query=None):\n",
    "    vocab = {} #make new dict in format of {word: doc1, doc10, doc7    word2: doc 8, doc 19,    wordN: etc.}\n",
    "    for i, word in enumerate(indexlist): #i = index, word is list of ['~', '598'] (words_after), indexlist = words_after\n",
    "        if word[0] in vocab: #word[0] is the character/word/feature, check if already in vocab dict\n",
    "            vocab.get(word[0]).append(word[1]) #if so, get the value of that word and append the new doc# to it\n",
    "        else:\n",
    "            vocab[word[0]] = [word[1]] #else, make a new dict entry with that word and set value = to doc#\n",
    "            \n",
    "    # stop_word processing if the 2nd parameter was given 'True'    \n",
    "    if stop_words_processing == True:\n",
    "        stopwords = stop_words(vocab, 10) #pass in your constructed vocab and desired top 10 most words\n",
    "        for k, v in list(vocab.items()): #traverse vocab dict, k = words, v = doc ID #s\n",
    "            if k in stopwords: #if vocab word in stopwords\n",
    "                del vocab[k] #delete the vocab dict entry of that vocab word    \n",
    "   \n",
    "\n",
    "    if query:\n",
    "        for k, v in list(vocab.items()): #traverse vocab dict, k = words, v = doc ID #s\n",
    "            if k == query: #if vocab word is the word you passed thru into function\n",
    "                print(f\"The doc ID(s) for your query '{k}' is   {v}\") #print word and word's value list of doc IDs\n",
    "                return\n",
    "    \n",
    "    # sum of the length of the postings lists \n",
    "    sum_len_values = sum(len(dct) for dct in vocab.values())           \n",
    "    return len(vocab), sum_len_values # len(vocab) for the size of the dictionary     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-rochester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3684, 50966)\n",
      "(3674, 43163)\n",
      "The doc ID(s) for your query 'school' is   ['72', '111', '223', '224', '268', '385', '431', '494', '532', '553', '554', '564', '581', '582', '996']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# before stop words processing \n",
    "print(inverted_index(words_after, False))\n",
    "\n",
    "# after stop words processing \n",
    "print(inverted_index(words_after, True))\n",
    "\n",
    "# simple query \n",
    "print(inverted_index(words_after, True, 'school'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "capital-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function that performs the intersection \n",
    "between two or more posting lists \n",
    "'''\n",
    "from itertools import zip_longest\n",
    "\n",
    "def intersect(lst1, lst2):\n",
    "    # e.g. str '2' -> int 2 \n",
    "    lst1 = list(map(int, lst1))\n",
    "    lst2 = list(map(int, lst2))\n",
    "    \n",
    "    res = [];\n",
    "    i=0 ; j = 0; compare_count = 0\n",
    "\n",
    "    while i<len(lst1) and j<len(lst2):\n",
    "        if lst1[i] == lst2[j]:\n",
    "            res.append(lst1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "            compare_count += 1\n",
    "        \n",
    "        elif lst1[i] < lst2[j]:\n",
    "            i += 1\n",
    "            compare_count += 1 \n",
    "        \n",
    "        elif lst1[i]:\n",
    "            j += 1\n",
    "            compare_count += 1 \n",
    "\n",
    "    print(\"No.of comparison: \", compare_count)\n",
    "    return res\n",
    "\n",
    "\n",
    "def intersect_w_skip(lst1, lst2):\n",
    "    lst1 = list(map(int, lst1))\n",
    "    lst2 = list(map(int, lst2))\n",
    "    \n",
    "    res = [];\n",
    "    i=0 ; j = 0; compare_count = 0;\n",
    "    \n",
    "    while i<len(lst1) and j<len(lst2):\n",
    "        if lst1[i] == lst2[j]:\n",
    "            res.append(lst1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "            compare_count += 1\n",
    "        \n",
    "        elif lst1[i] < lst2[j]:\n",
    "            i += 1\n",
    "            compare_count += 1 \n",
    "        \n",
    "        elif lst1[i]:\n",
    "            j += 1\n",
    "            compare_count += 1 \n",
    "\n",
    "    print(\"No.of comparison: \", compare_count)\n",
    "    return res\n",
    "\n",
    "\n",
    "def query(indexlist):\n",
    "    vocab = {} #make a vocab dict of the indexlist\n",
    "    for i, word in enumerate(indexlist):\n",
    "        if word[0] in vocab:\n",
    "            vocab.get(word[0]).append(word[1])       \n",
    "        else:\n",
    "            vocab[word[0]] = [word[1]]\n",
    "    \n",
    "    stopwords = stop_words(vocab, 10) #get top 10 stopwords\n",
    "    \n",
    "    for k, v in list(vocab.items()): #remove stopwords\n",
    "        if k in stopwords:\n",
    "            del vocab[k]    \n",
    "    \n",
    "    lst = []; #new list to hold doc IDs\n",
    "    \n",
    "    user_input = input(\"Enter an intersection query (format: a AND b): \") #ask user input\n",
    "    user_input = user_input.lower() #to account for user not capitalizing 'AND' condition\n",
    "    input_list = user_input.split(' and ') #split user input into list around the 'and' condition. result is list of words they want to intersect\n",
    "    \n",
    "\n",
    "    for word in input_list: #check if all user input words are in the vocab\n",
    "        if word not in vocab:\n",
    "            print(f\"Query entry '{word}' not in vocabulary. Try again.\")\n",
    "            return\n",
    "    \n",
    "    for i, query in enumerate(input_list): #traverse thru user list, i =index of user list, query = word user asking for\n",
    "        for k, v in list(vocab.items()): #traverse thru vocab words, k = vocab word, v = doc IDs\n",
    "            if k == query: #if vocab word == word user wants\n",
    "                lst.append(v) #append doc IDs into list\n",
    "    \n",
    "    sorted_list = sorted(lst, key=len) #sort by the length of the # of docs each word has\n",
    "    res = intersect(sorted_list[0], sorted_list[1]) #get interest of both words\n",
    "    \n",
    "    if len(lst) == 2: #if only 2 entries\n",
    "        return res #return intersection\n",
    "\n",
    "    else: #more than 2 words\n",
    "        k = 2\n",
    "        while k < len(sorted_list)-1:\n",
    "            res = intersect(res, sorted_list[k]) #compare the first 2 words' intersection with the next one until you're at the max words\n",
    "            k += 1 #increment k\n",
    "            \n",
    "        return res #return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "silent-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an intersection query (format: a AND b):  school and kids\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 32\n",
      "111 72\n",
      "223 175\n",
      "224 224\n",
      "268 336\n",
      "385 385\n",
      "431 459\n",
      "494 460\n",
      "532 475\n",
      "553 523\n",
      "554 538\n",
      "564 539\n",
      "581 553\n",
      "582 583\n",
      "996 584\n",
      "None 593\n",
      "None 613\n",
      "None 786\n",
      "None 806\n",
      "None 970\n"
     ]
    }
   ],
   "source": [
    "# school AND kids AND really\n",
    "query(words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "million-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranked queries for the query 'school AND kids AND really' \n",
    "\n",
    "with open('index_rank.txt', encoding = \"utf8\") as f:\n",
    "    # after lower casing \n",
    "    words_after_rank = [line.split() for line in f.read().split('\\n') if line]\n",
    "    \n",
    "    \n",
    "for i in range(len(words_after)):\n",
    "    words_after_rank[i][0] = int(words_after_rank[i][0])\n",
    "    words_after_rank[i][2] = int(words_after_rank[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "genetic-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polyphonic-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_total_doc = 3 \n",
    "doc_id = ['72', '224', '385']\n",
    "\n",
    "\n",
    "vocab = []\n",
    "\n",
    "for i in words_after_rank:\n",
    "    if i[2] == 72:\n",
    "        vocab.append(i[1])\n",
    "        \n",
    "    elif i[2] == 224:\n",
    "        vocab.append(i[1])\n",
    "        \n",
    "    elif i[2] == 385:\n",
    "        vocab.append(i[1])\n",
    "        \n",
    "vocab = sorted(list(set(vocab)))\n",
    "\n",
    "df1 = pd.DataFrame({'doc_id': doc_id})\n",
    "df = pd.DataFrame(vocab).T\n",
    "df = df.rename(columns=df.iloc[0])\n",
    "df = df.drop(df.index[0])\n",
    "df = pd.concat([df1, df])\n",
    "\n",
    " \n",
    "\n",
    "for i in words_after_rank:\n",
    "    if i[2] == 72:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exact-running",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'</th>\n",
       "      <th>,</th>\n",
       "      <th>-</th>\n",
       "      <th>.</th>\n",
       "      <th>...</th>\n",
       "      <th>?</th>\n",
       "      <th>a</th>\n",
       "      <th>always</th>\n",
       "      <th>anyone</th>\n",
       "      <th>bad</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>well</th>\n",
       "      <th>what'</th>\n",
       "      <th>you</th>\n",
       "      <th>you'</th>\n",
       "      <th>–</th>\n",
       "      <th>“</th>\n",
       "      <th>”</th>\n",
       "      <th>…</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     '    ,    -    .  ...    ?    a always anyone  bad  ...  was   we well  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN  ...  NaN  NaN  NaN   \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN    NaN    NaN  NaN  ...  NaN  NaN  NaN   \n",
       "\n",
       "  what'  you you'    –    “    ”    …  \n",
       "0   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[3 rows x 74 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. skip pointer \n",
    "# 2. tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-staff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
